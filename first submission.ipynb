{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/application_train.csv')\n",
    "test = pd.read_csv('./input/application_test.csv')\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "buro = pd.read_csv('./input/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = [\n",
    "    f for f in train.columns if train[f].dtype == 'object'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in categorical_feats:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['TARGET']\n",
    "del train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    prev[f_], _ = pd.factorize(prev[f_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buro_cat_features = [\n",
    "    f_ for f_ in buro.columns if buro[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in buro_cat_features:\n",
    "    buro[f_], _ = pd.factorize(buro[f_])\n",
    "\n",
    "avg_buro = buro.groupby('SK_ID_CURR').mean()\n",
    "avg_buro['buro_count'] = buro[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "del avg_buro['SK_ID_BUREAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "train = train.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_feats = ['SK_ID_CURR']\n",
    "features = [f_ for f_ in train.columns if f_ not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.770254\tvalid_1's auc: 0.748156\n",
      "[500]\ttraining's auc: 0.789974\tvalid_1's auc: 0.757464\n",
      "[750]\ttraining's auc: 0.806531\tvalid_1's auc: 0.764052\n",
      "[1000]\ttraining's auc: 0.819404\tvalid_1's auc: 0.76797\n",
      "[1250]\ttraining's auc: 0.829609\tvalid_1's auc: 0.770191\n",
      "[1500]\ttraining's auc: 0.838101\tvalid_1's auc: 0.77162\n",
      "[1750]\ttraining's auc: 0.84601\tvalid_1's auc: 0.772623\n"
     ]
    }
   ],
   "source": [
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=20000,\n",
    "        learning_rate=0.005,\n",
    "        num_leaves=70,\n",
    "        colsample_bytree=.8,\n",
    "        subsample=.9,\n",
    "        max_depth=7,\n",
    "        reg_alpha=.1,\n",
    "        reg_lambda=.1,\n",
    "        min_split_gain=.01,\n",
    "        min_child_weight=2,\n",
    "        device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    clf.fit(trn_x, trn_y, \n",
    "            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "            eval_metric='auc', verbose=250, early_stopping_rounds=150\n",
    "           )\n",
    "    \n",
    "    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "    \n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    del clf, trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()\n",
    "    \n",
    "print('Full AUC score %.6f' % roc_auc_score(y, oof_preds))   \n",
    "\n",
    "test['TARGET'] = sub_preds\n",
    "\n",
    "test[['SK_ID_CURR', 'TARGET']].to_csv('submits/first_submission.csv', index=False, float_format='%.8f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

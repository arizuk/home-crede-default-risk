{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n"
     ]
    }
   ],
   "source": [
    " train = utils.read_csv('./input/application_train.csv')\n",
    " test = utils.read_csv('./input/application_test.csv')\n",
    " prev = utils.read_csv('./input/previous_application.csv')\n",
    " buro = utils.read_csv('./input/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = [\n",
    "    f for f in train.columns if train[f].dtype == 'object'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in categorical_feats:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['TARGET']\n",
    "del train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    prev[f_], _ = pd.factorize(prev[f_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buro_cat_features = [\n",
    "    f_ for f_ in buro.columns if buro[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in buro_cat_features:\n",
    "    buro[f_], _ = pd.factorize(buro[f_])\n",
    "\n",
    "avg_buro = buro.groupby('SK_ID_CURR').mean()\n",
    "avg_buro['buro_count'] = buro[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "del avg_buro['SK_ID_BUREAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "train = train.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_feats = ['SK_ID_CURR']\n",
    "features = [f_ for f_ in train.columns if f_ not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x, val_x, trn_y, val_y = train_test_split(train[features], y,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.701239\tvalid_1's auc: 0.696289\n",
      "AUC : 0.696289\n",
      "Save to experiments/003-lgbm-0.696289.csv\n"
     ]
    }
   ],
   "source": [
    "exp_id = utils.experiment_id()\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    n_estimators=20000,\n",
    "    learning_rate=0.005,\n",
    "    num_leaves=70,\n",
    "    colsample_bytree=.8,\n",
    "    subsample=.9,\n",
    "    max_depth=7,\n",
    "    reg_alpha=.1,\n",
    "    reg_lambda=.1,\n",
    "    min_split_gain=.01,\n",
    "    min_child_weight=2,\n",
    "    device=\"gpu\"\n",
    ")\n",
    "\n",
    "clf.fit(trn_x, trn_y, \n",
    "        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "        eval_metric='auc', verbose=250, early_stopping_rounds=150\n",
    "       )\n",
    "\n",
    "val_preds = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "test_preds = clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(val_y, val_preds)\n",
    "print('AUC : %.6f' % auc)\n",
    "gc.collect()\n",
    "\n",
    "csv = f\"experiments/{exp_id}-lgbm-{auc:.6f}.csv\"\n",
    "\n",
    "test['TARGET'] = test_preds\n",
    "test[['SK_ID_CURR', 'TARGET']].to_csv(csv, index=False, float_format='%.8f')\n",
    "print(f'Save to {csv}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

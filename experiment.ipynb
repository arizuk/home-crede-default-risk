{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id():\n",
    "    files = [f for f in os.listdir('./submits/experiments')]\n",
    "    files.sort()\n",
    "    files.reverse()\n",
    "    for f in files:\n",
    "        m = re.match('(\\d+).csv', f)\n",
    "        if m:\n",
    "            last_id = int(m.group(1)) + 1\n",
    "            return f\"{last_id:03d}\"\n",
    "    return '001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/application_train.csv')\n",
    "test = pd.read_csv('./input/application_test.csv')\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "buro = pd.read_csv('./input/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = [\n",
    "    f for f in train.columns if train[f].dtype == 'object'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in categorical_feats:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['TARGET']\n",
    "del train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    prev[f_], _ = pd.factorize(prev[f_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buro_cat_features = [\n",
    "    f_ for f_ in buro.columns if buro[f_].dtype == 'object'\n",
    "]\n",
    "for f_ in buro_cat_features:\n",
    "    buro[f_], _ = pd.factorize(buro[f_])\n",
    "\n",
    "avg_buro = buro.groupby('SK_ID_CURR').mean()\n",
    "avg_buro['buro_count'] = buro[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "del avg_buro['SK_ID_BUREAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "train = train.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_feats = ['SK_ID_CURR']\n",
    "features = [f_ for f_ in train.columns if f_ not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x, val_x, trn_y, val_y = train_test_split(train[features], y,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[250]\ttraining's auc: 0.770247\tvalid_1's auc: 0.747771\n",
      "[500]\ttraining's auc: 0.789973\tvalid_1's auc: 0.757256\n",
      "[750]\ttraining's auc: 0.806526\tvalid_1's auc: 0.763826\n",
      "[1000]\ttraining's auc: 0.819141\tvalid_1's auc: 0.767792\n",
      "[1250]\ttraining's auc: 0.829232\tvalid_1's auc: 0.770017\n",
      "[1500]\ttraining's auc: 0.837954\tvalid_1's auc: 0.771599\n",
      "[1750]\ttraining's auc: 0.845771\tvalid_1's auc: 0.77263\n",
      "[2000]\ttraining's auc: 0.852802\tvalid_1's auc: 0.773369\n",
      "[2250]\ttraining's auc: 0.859448\tvalid_1's auc: 0.773979\n",
      "[2500]\ttraining's auc: 0.865678\tvalid_1's auc: 0.774465\n",
      "[2750]\ttraining's auc: 0.871486\tvalid_1's auc: 0.774866\n",
      "[3000]\ttraining's auc: 0.876922\tvalid_1's auc: 0.77508\n",
      "[3250]\ttraining's auc: 0.882308\tvalid_1's auc: 0.775185\n",
      "[3500]\ttraining's auc: 0.887319\tvalid_1's auc: 0.77531\n",
      "[3750]\ttraining's auc: 0.892219\tvalid_1's auc: 0.775422\n",
      "[4000]\ttraining's auc: 0.896777\tvalid_1's auc: 0.775366\n",
      "Early stopping, best iteration is:\n",
      "[3864]\ttraining's auc: 0.894317\tvalid_1's auc: 0.77546\n",
      "AUC : 0.775460\n",
      "Save to submits/experiments/001.csv\n"
     ]
    }
   ],
   "source": [
    "csv = f\"submits/experiments/{get_experiment_id()}.csv\"\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    n_estimators=20000,\n",
    "    learning_rate=0.005,\n",
    "    num_leaves=70,\n",
    "    colsample_bytree=.8,\n",
    "    subsample=.9,\n",
    "    max_depth=7,\n",
    "    reg_alpha=.1,\n",
    "    reg_lambda=.1,\n",
    "    min_split_gain=.01,\n",
    "    min_child_weight=2,\n",
    "    device=\"gpu\"\n",
    ")\n",
    "\n",
    "clf.fit(trn_x, trn_y, \n",
    "        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "        eval_metric='auc', verbose=250, early_stopping_rounds=150\n",
    "       )\n",
    "\n",
    "val_preds = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "test_preds = clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "print('AUC : %.6f' % (roc_auc_score(val_y, val_preds)))\n",
    "gc.collect()\n",
    "    \n",
    "test['TARGET'] = test_preds\n",
    "test[['SK_ID_CURR', 'TARGET']].to_csv(csv, index=False, float_format='%.8f')\n",
    "print(f'Save to {csv}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
